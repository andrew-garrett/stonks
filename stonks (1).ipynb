{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost\n",
    "!pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import linear_model\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import r2_score, accuracy_score, f1_score, mean_squared_error, plot_confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "from scipy.stats import randint, uniform\n",
    "import shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "compustat1990 = pd.read_csv('C:\\\\Users\\\\andre\\\\OneDrive\\\\Documents\\\\SchoolWork\\\\2021 Spring Semester\\\\CIS 519\\\\Final Project\\\\compustat1990-2000.csv', low_memory=False)\n",
    "compustat2000 = pd.read_csv('C:\\\\Users\\\\andre\\\\OneDrive\\\\Documents\\\\SchoolWork\\\\2021 Spring Semester\\\\CIS 519\\\\Final Project\\\\compustat2000-2010.csv', low_memory=False)\n",
    "compustat2010 = pd.read_csv('C:\\\\Users\\\\andre\\\\OneDrive\\\\Documents\\\\SchoolWork\\\\2021 Spring Semester\\\\CIS 519\\\\Final Project\\\\compustat2010-2021.csv', low_memory=False)\n",
    "\n",
    "# Import intermediate/joiner data\n",
    "compustatjoin = pd.read_csv('C:\\\\Users\\\\andre\\\\OneDrive\\\\Documents\\\\SchoolWork\\\\2021 Spring Semester\\\\CIS 519\\\\Final Project\\\\compustatjoin.csv', low_memory=False)\n",
    "ibesjoin = pd.read_csv('C:\\\\Users\\\\andre\\\\OneDrive\\\\Documents\\\\SchoolWork\\\\2021 Spring Semester\\\\CIS 519\\\\Final Project\\\\ibesjoin.csv', low_memory=False)\n",
    "\n",
    "# Import forecast data\n",
    "ibes1990 = pd.read_csv('C:\\\\Users\\\\andre\\\\OneDrive\\\\Documents\\\\SchoolWork\\\\2021 Spring Semester\\\\CIS 519\\\\Final Project\\\\ibes1990-2000.csv', low_memory=False)\n",
    "ibes2000 = pd.read_csv('C:\\\\Users\\\\andre\\\\OneDrive\\\\Documents\\\\SchoolWork\\\\2021 Spring Semester\\\\CIS 519\\\\Final Project\\\\ibes2000-2010.csv', low_memory=False)\n",
    "ibes2010 = pd.read_csv('C:\\\\Users\\\\andre\\\\OneDrive\\\\Documents\\\\SchoolWork\\\\2021 Spring Semester\\\\CIS 519\\\\Final Project\\\\ibes2010-2020.csv', low_memory=False)\n",
    "\n",
    "# Concatenate timeseries data\n",
    "compustat_df = pd.concat([compustat1990, compustat2000, compustat2010])\n",
    "ibes_df = pd.concat([ibes1990, ibes2000, ibes2010])\n",
    "\n",
    "# Merge data and forecasts according to appropriate naming conventions\n",
    "compustat_df = compustat_df.merge(compustatjoin,on='gvkey',how='inner')\n",
    "ibes_df = ibes_df.merge(ibesjoin,on='OFTIC',how='inner')\n",
    "\n",
    "# Filter for only quarterly reports\n",
    "quarterly_ibes = ibes_df[ibes_df['FISCALP'] == 'QTR']\n",
    "\n",
    "# Merge forecasts and data\n",
    "merged_df = compustat_df.merge(quarterly_ibes, left_on=['permno','datadate'], right_on=['PERMNO','FPEDATS'], how='inner')\n",
    "\n",
    "# Save merged data locally\n",
    "# compression_opts = dict(method='zip',\n",
    "#                         archive_name='out.csv')  \n",
    "# merged_df.to_csv('C:\\\\Users\\\\andre\\\\OneDrive\\\\Documents\\\\SchoolWork\\\\2021 Spring Semester\\\\CIS 519\\\\Final Project\\\\merged_data.zip', index=False, compression=compression_opts)\n",
    "\n",
    "# Read merged_data.zip (saved above)\n",
    "# import zipfile\n",
    "# zf = zipfile.ZipFile('C:\\\\Users\\\\andre\\\\OneDrive\\\\Documents\\\\SchoolWork\\\\2021 Spring Semester\\\\CIS 519\\\\Final Project\\\\merged_data.zip')\n",
    "# merged_df = pd.read_csv(zf.open('out.csv'))\n",
    "\n",
    "# Add Ground Truth\n",
    "merged_df = merged_df.dropna(subset=['atq','MEANEST','ACTUAL'])\n",
    "merged_df = merged_df[merged_df['atq'] != 0]\n",
    "merged_df['AFE'] = ((merged_df['ACTUAL'] - merged_df['MEANEST'])*merged_df['cshoq'])\n",
    "\n",
    "# Allocate memory\n",
    "del compustat1990, compustat2000, compustat2010\n",
    "del compustatjoin, ibesjoin\n",
    "del ibes1990, ibes2000, ibes2010\n",
    "del compustat_df, quarterly_ibes       \n",
    "\n",
    "# Clean data\n",
    "cleaned_df = merged_df.dropna(axis=1, how='all')\n",
    "cleaned_df = cleaned_df.dropna(subset=['AFE'])\n",
    "cleaned_df = cleaned_df.drop(columns=ibes_df.columns)\n",
    "cleaned_df = cleaned_df.drop(columns=['gvkey','indfmt','cusip','tic','datadate','fyearq','fqtr','fyr','conm','cik','exchg','fic','fyrc','datacqtr','datafqtr','rdq'])\n",
    "for column in cleaned_df.columns:\n",
    "    if (column != 'atq' and column != 'permno'):\n",
    "        cleaned_df[column] = cleaned_df[column]/cleaned_df['atq']\n",
    "\n",
    "# Remove outliers\n",
    "cleaned_df = cleaned_df[np.abs(cleaned_df['AFE']) < 10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression on AFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing (grouped by company)\n",
    "permno_df = cleaned_df['permno'].drop_duplicates().reset_index()['permno']\n",
    "np.random.seed(42)\n",
    "train_permnos = np.random.choice(permno_df, 4708, replace=False)\n",
    "train_df = cleaned_df[cleaned_df['permno'].isin(train_permnos)]\n",
    "test_df = cleaned_df[~cleaned_df['permno'].isin(train_permnos)]\n",
    "train_df = train_df.drop(columns=['permno'])\n",
    "test_df = test_df.drop(columns=['permno'])\n",
    "\n",
    "# Allocate more memory\n",
    "del ibes_df\n",
    "del merged_df, permno_df, train_permnos\n",
    "                          \n",
    "# NaN imputation\n",
    "to_remove = []\n",
    "threshold = 0.5\n",
    "for column in train_df.columns:\n",
    "    \n",
    "    missing_vals = train_df[column].isna().sum() + test_df[column].isna().sum()\n",
    "    pct = missing_vals / (len(train_df[column]) + len(test_df[column]))\n",
    "    \n",
    "    if ((np.all(np.isnan(train_df[column])) or np.all(np.isnan(test_df[column]))) or pct >= threshold):\n",
    "        to_remove.append(column)\n",
    "    \n",
    "    # Training set imputation\n",
    "    train_mean = train_df[column].mean()\n",
    "    train_df[column] = train_df[column].fillna(train_mean)\n",
    "\n",
    "    # Testing set imputation\n",
    "    test_mean = test_df[column].mean()\n",
    "    test_df[column] = test_df[column].fillna(test_mean)\n",
    "\n",
    "# Drop empty columns\n",
    "train_df = train_df.drop(columns=to_remove)\n",
    "test_df = test_df.drop(columns=to_remove)\n",
    "\n",
    "# Split data into features and labels, then standardize according to training\n",
    "y_train = train_df['AFE']\n",
    "X_train = train_df.drop('AFE',axis=1)\n",
    "y_test = test_df['AFE']\n",
    "X_test = test_df.drop('AFE',axis=1)\n",
    "\n",
    "standardizer = StandardScaler()\n",
    "X_train = pd.DataFrame(standardizer.fit_transform(X_train))\n",
    "X_test = pd.DataFrame(standardizer.transform(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression model\n",
    "lr_model = linear_model.LinearRegression()\n",
    "lr_model.fit(X_train.to_numpy(),y_train.to_numpy())\n",
    "y_hat_train = lr_model.predict(X_train.to_numpy())\n",
    "train_score = r2_score(y_train.to_numpy(), y_hat_train)\n",
    "print('Training R^2 Score (vanilla): ' + str(train_score))\n",
    "\n",
    "# Evaluate linear model for all features on test data\n",
    "y_hat_test = lr_model.predict(X_test.to_numpy())\n",
    "test_score = r2_score(y_test.to_numpy(), y_hat_test)\n",
    "print('Testing R^2 Score (vanilla): ' + str(test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA (attempt 1)\n",
    "pca = PCA(n_components=X_train.shape[1])\n",
    "pca.fit_transform(X_train)\n",
    "pc_vs_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# Plot cumulative variance as a function of PC's\n",
    "plt.plot(pc_vs_variance)\n",
    "plt.title('Principal Components vs Explained Variance')\n",
    "plt.xlabel('No. of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "\n",
    "# Select components which encompass 95% of explained variance\n",
    "pca = PCA(n_components=np.argmax(pc_vs_variance>0.95))\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "# Train linear model for PCs\n",
    "lr_model_pca = linear_model.LinearRegression()\n",
    "lr_model_pca.fit(X_train_pca,y_train.to_numpy())\n",
    "y_hat_train_pca = lr_model_pca.predict(X_train_pca)\n",
    "train_score_pca = r2_score(y_train.to_numpy(), y_hat_train_pca)\n",
    "print('Training R^2 Score (PCA): ' + str(train_score_pca))\n",
    "\n",
    "# Evaluate linear model for PCs on test data (transformed)\n",
    "y_hat_test_pca = lr_model_pca.predict(X_test_pca)\n",
    "test_score_pca = r2_score(y_test.to_numpy(), y_hat_test_pca)\n",
    "print('Testing R^2 Score (PCA): ' + str(test_score_pca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA (attempt 2)\n",
    "pca = PCA()\n",
    "X_reduced_train = pca.fit_transform(X_train)\n",
    "n = len(X_reduced_train)\n",
    "\n",
    "mse = []\n",
    "R2 = []\n",
    "\n",
    "# Calculate MSE with only the intercept (no principal components in regression)\n",
    "y_train_np = y_train.to_numpy()\n",
    "\n",
    "# Calculate MSE using CV for the 19 principle components, adding one component at the time.\n",
    "for i in np.arange(1, int(0.5*X_train.shape[1])):\n",
    "    regr = linear_model.LinearRegression()\n",
    "    regr.fit(X_reduced_train[:,:i], y_train_np)\n",
    "    y_hat_trained = regr.predict(X_reduced_train[:,:i])\n",
    "    \n",
    "    mse_score = mean_squared_error(y_train_np, y_hat_trained)\n",
    "    R2_score = r2_score(y_train_np, y_hat_trained)\n",
    "    mse.append(mse_score)\n",
    "    R2.append(R2_score)\n",
    "    if (i % 10 == 0):\n",
    "        print('No. of PCs: ' + str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison MSE as a function of number of PCs\n",
    "mse = np.array(mse)\n",
    "plt.plot(mse, '-v')\n",
    "plt.xlabel('Number of principal components in regression')\n",
    "plt.ylabel('MSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison R2 as a function of number of PCs\n",
    "R2 = np.array(R2)\n",
    "plt.plot(R2, '-v')\n",
    "plt.xlabel('Number of principal components in regression')\n",
    "plt.ylabel('R2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use generated PCA transformation on test data\n",
    "test_mse = []\n",
    "test_R2 = []\n",
    "\n",
    "y_test_np = np.array(y_test)\n",
    "\n",
    "for i in range(1, int(0.5*X_train.shape[1])):\n",
    "    X_reduced_test = pca.transform(X_test)[:,:i]\n",
    "\n",
    "    # Train regression model on training data \n",
    "    regr = linear_model.LinearRegression()\n",
    "    regr.fit(X_reduced_train[:,:i], y_train)\n",
    "\n",
    "    # Prediction with test data\n",
    "    pred = regr.predict(X_reduced_test)\n",
    "    \n",
    "    test_mse_score = mean_squared_error(y_test_np, pred)\n",
    "    test_R2_score = r2_score(y_test_np, pred)\n",
    "    test_mse.append(test_mse_score)\n",
    "    test_R2.append(test_R2_score)\n",
    "    if (i % 10 == 0):\n",
    "        print('No. of PCs: ' + str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison test-MSE as a function of number of PCs\n",
    "test_mse = np.array(test_mse)\n",
    "plt.plot(test_mse, '-v')\n",
    "plt.xlabel('Number of principal components in regression')\n",
    "plt.ylabel('test MSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison test-R2 as a function of number of PCs\n",
    "test_R2 = np.array(test_R2)\n",
    "plt.plot(test_R2, '-v')\n",
    "plt.xlabel('Number of principal components in regression')\n",
    "plt.ylabel('test R2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of AFE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deciles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split AFE into deciles\n",
    "classification_df = cleaned_df\n",
    "classification_df['AFE_decile'] = pd.qcut(classification_df['AFE'], 10, labels = False)\n",
    "classification_df = classification_df.drop(columns=['AFE'])\n",
    "\n",
    "# Split data into training and testing (grouped by company)\n",
    "permno_df = classification_df['permno'].drop_duplicates().reset_index()['permno']\n",
    "np.random.seed(42)\n",
    "train_permnos = np.random.choice(permno_df, 4708, replace=False)\n",
    "train_df = classification_df[classification_df['permno'].isin(train_permnos)]\n",
    "test_df = classification_df[~classification_df['permno'].isin(train_permnos)]\n",
    "train_df = train_df.drop(columns=['permno'])\n",
    "test_df = test_df.drop(columns=['permno'])\n",
    "\n",
    "# NaN imputation\n",
    "to_remove = []\n",
    "threshold = 0.5\n",
    "for column in train_df.columns:\n",
    "    \n",
    "    missing_vals = train_df[column].isna().sum() + test_df[column].isna().sum()\n",
    "    pct = missing_vals / (len(train_df[column]) + len(test_df[column]))\n",
    "    \n",
    "    if ((np.all(np.isnan(train_df[column])) or np.all(np.isnan(test_df[column]))) or pct >= threshold):\n",
    "        to_remove.append(column)\n",
    "    \n",
    "    # Training set imputation\n",
    "    train_mean = train_df[column].mean()\n",
    "    train_df[column] = train_df[column].fillna(train_mean)\n",
    "\n",
    "    # Testing set imputation\n",
    "    test_mean = test_df[column].mean()\n",
    "    test_df[column] = test_df[column].fillna(test_mean)\n",
    "\n",
    "# Drop empty columns\n",
    "train_df = train_df.drop(columns=to_remove)\n",
    "test_df = test_df.drop(columns=to_remove)\n",
    "\n",
    "# Split data into features and labels, then standardize according to training\n",
    "y_train = train_df['AFE_decile']\n",
    "X_train = train_df.drop('AFE_decile',axis=1)\n",
    "y_test = test_df['AFE_decile']\n",
    "X_test = test_df.drop('AFE_decile',axis=1)\n",
    "\n",
    "standardizer = StandardScaler()\n",
    "X_train = pd.DataFrame(standardizer.fit_transform(X_train))\n",
    "X_test = pd.DataFrame(standardizer.transform(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search on depth and number of estimators (Random Forests)\n",
    "param_grid_decile = {\n",
    "    \"n_estimators\": [100, 200, 300, 500],\n",
    "    \"max_depth\": [3, 5, 7, 9]\n",
    "}\n",
    "clf_decile = RandomForestClassifier()\n",
    "grid_search_decile = GridSearchCV(estimator = clf_decile, param_grid = param_grid_decile, n_jobs = -1)\n",
    "grid_search_decile.fit(X_train, y_train)\n",
    "opt_params_decile_rf = grid_search_decile.best_params_\n",
    "print(opt_params_decile_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Random Forest implementation\n",
    "# clf_decile_rf = RandomForestClassifier(n_estimators=opt_params_decile_rf['n_estimators'], max_depth=opt_params_decile_rf['max_depth'], random_state=42)\n",
    "clf_decile_rf = RandomForestClassifier(n_estimators=500, max_depth=9, random_state=42)\n",
    "clf_decile_rf.fit(X_train, y_train)\n",
    "y_hat_train = clf_decile_rf.predict(X_train)\n",
    "y_hat_test = clf_decile_rf.predict(X_test)\n",
    "train_score = accuracy_score(y_train, y_hat_train)\n",
    "test_score = accuracy_score(y_test, y_hat_test)\n",
    "print(\"Tuned Random Forest (by decile) Train Score: \" + str(train_score))\n",
    "print(\"Tuned Random Forest (by decile) Test Score: \" + str(test_score))\n",
    "\n",
    "'''\n",
    "# Feature importance\n",
    "importances_decile = clf_decile_rf.feature_importances_\n",
    "std_decile = np.std([tree.feature_importances_ for tree in clf_decile_rf.estimators_],\n",
    "             axis=0)\n",
    "indices_decile = np.argsort(importances_decile)[::-1]\n",
    "plt.figure(figsize=(10, 8), dpi=80)\n",
    "plt.title(\"Feature importances - Tuned RF (Deciles)\")\n",
    "first = 30\n",
    "plt.bar(range(first), importances_decile[indices_decile[:first]],\n",
    "        color=\"r\", yerr=std_decile[indices_decile[:first]], align=\"center\")\n",
    "plt.xticks(range(first), indices_decile[:first-1])\n",
    "plt.xlim([-1, first])\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Randomized Grid Search on depth and number of estimators (xgboost)\n",
    "\n",
    "param_distributions_decile = {\n",
    "    \"n_estimators\": [100, 200, 300, 400, 500],\n",
    "    \"max_depth\": randint(low=1, high=10),\n",
    "    \"learning_rate\": [0.1, 0.25, 0.5, 0.75],\n",
    "    \"subsample\": [0.6, 0.8, 1.0]\n",
    "}\n",
    "xgb_decile = XGBClassifier(objective='multi:softmax', nthread=4, random_state=42)\n",
    "rand_search_decile = RandomizedSearchCV(estimator = xgb_decile, param_distributions = param_distributions_decile, n_iter=100, n_jobs = -1, random_state=42, verbose=2)\n",
    "rand_search_decile.fit(X_train, y_train)\n",
    "opt_params_decile_xgb = rand_search_decile.best_params_\n",
    "print(opt_params_decile_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost implementation\n",
    "'''\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "param = {'max_depth': opt_params_decile_xgb['max_depth'], 'learning_rate': opt_params_decile_xgb['learning_rate'], 'objective': 'multi:softmax', }\n",
    "param['nthread'] = 4\n",
    "param['eval_metric'] = ['merror']\n",
    "evallist = [(dtest, 'eval'), (dtrain, 'train')]\n",
    "\n",
    "num_round = 100\n",
    "bst = xgb.train(param, dtrain, num_round, evallist)\n",
    "bst.save_model('0003.model')\n",
    "y_hat_test = bst.predict(dtest)\n",
    "y_hat_train = bst.predict(dtrain)\n",
    "print()\n",
    "print(\"XGBoost (by decile) Train Score: \" + str(accuracy_score(y_train, y_hat_train)))\n",
    "print(\"XGBoost (by decile) Test Score: \" + str(accuracy_score(y_test,y_hat_test)))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MultiLayer Perceptron (currently untuned)\n",
    "'''\n",
    "clf = MLPClassifier(hidden_layer_sizes=(256,256), activation='tanh', solver='adam', \n",
    "                    max_iter=100, random_state=42, verbose=True, early_stopping=True).fit(X_train, y_train)\n",
    "y_hat_test = clf.predict(X_test)\n",
    "y_hat_train = clf.predict(X_train)\n",
    "train_score = clf.score(X_train, y_train)\n",
    "test_score = clf.score(X_test, y_test)\n",
    "print(train_score)\n",
    "print(test_score)\n",
    "plt.plot(clf.loss_curve_)\n",
    "plt.title('Log-Loss over time for MLP (decile)')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Log-Loss')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tertiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split AFE into tertiles\n",
    "classification_df = cleaned_df\n",
    "classification_df['AFE_tertile'] = pd.qcut(classification_df['AFE'], 3, labels = False)\n",
    "classification_df = classification_df.drop(columns=['AFE'])\n",
    "\n",
    "# Split data into training and testing (grouped by company)\n",
    "permno_df = classification_df['permno'].drop_duplicates().reset_index()['permno']\n",
    "np.random.seed(26)\n",
    "train_permnos = np.random.choice(permno_df, 4708, replace=False)\n",
    "train_df = classification_df[classification_df['permno'].isin(train_permnos)]\n",
    "test_df = classification_df[~classification_df['permno'].isin(train_permnos)]\n",
    "train_df = train_df.drop(columns=['permno'])\n",
    "test_df = test_df.drop(columns=['permno'])\n",
    "                          \n",
    "# NaN imputation\n",
    "to_remove = []\n",
    "threshold = 0.5\n",
    "for column in train_df.columns:\n",
    "    \n",
    "    missing_vals = train_df[column].isna().sum() + test_df[column].isna().sum()\n",
    "    pct = missing_vals / (len(train_df[column]) + len(test_df[column]))\n",
    "    \n",
    "    if ((np.all(np.isnan(train_df[column])) or np.all(np.isnan(test_df[column]))) or pct >= threshold):\n",
    "        to_remove.append(column)\n",
    "    \n",
    "    # Training set imputation\n",
    "    train_mean = train_df[column].mean()\n",
    "    train_df[column] = train_df[column].fillna(train_mean)\n",
    "\n",
    "    # Testing set imputation\n",
    "    test_mean = test_df[column].mean()\n",
    "    test_df[column] = test_df[column].fillna(test_mean)\n",
    "\n",
    "# Drop empty columns\n",
    "train_df = train_df.drop(columns=to_remove)\n",
    "test_df = test_df.drop(columns=to_remove)\n",
    "\n",
    "# Split data into features and labels, then standardize according to training\n",
    "y_train = train_df['AFE_tertile']\n",
    "X_train = train_df.drop('AFE_tertile',axis=1)\n",
    "y_test = test_df['AFE_tertile']\n",
    "X_test = test_df.drop('AFE_tertile',axis=1)\n",
    "\n",
    "standardizer = StandardScaler()\n",
    "X_train = pd.DataFrame(standardizer.fit_transform(X_train))\n",
    "X_test = pd.DataFrame(standardizer.transform(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search on depth and number of estimators (Random Forests)\n",
    "param_grid_tertile = {\n",
    "    \"n_estimators\": [100, 200, 300, 400, 500],\n",
    "    \"max_depth\": [3, 5, 7, 9]\n",
    "}\n",
    "clf_tertile = RandomForestClassifier()\n",
    "grid_search_tertile = GridSearchCV(estimator = clf_tertile, param_grid = param_grid_tertile, n_jobs = -1)\n",
    "grid_search_tertile.fit(X_train, y_train)\n",
    "opt_params_tertile_rf = grid_search_tertile.best_params_\n",
    "print(opt_params_tertile_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest implementation\n",
    "# clf_tertile_rf = RandomForestClassifier(n_estimators=opt_params_tertile_rf['n_estimators'], max_depth=opt_params_tertile_rf['max_depth'], random_state=42)\n",
    "clf_tertile_rf = RandomForestClassifier(n_estimators=400, max_depth=9, random_state=26)\n",
    "clf_tertile_rf.fit(X_train, y_train)\n",
    "y_hat_train = clf_tertile_rf.predict(X_train)\n",
    "y_hat_test = clf_tertile_rf.predict(X_test)\n",
    "train_score = accuracy_score(y_train, y_hat_train)\n",
    "test_score = accuracy_score(y_test, y_hat_test)\n",
    "print(\"Tuned Random Forest (by tertile) Train Score: \" + str(train_score))\n",
    "print(\"Tuned Random Forest (by tertile) Test Score: \" + str(test_score))\n",
    "\n",
    "# Confusion Matrix\n",
    "disp = plot_confusion_matrix(clf_tertile_rf, X_test, y_test, cmap=plt.cm.Blues,normalize='true')\n",
    "disp.ax_.set_title(\"Normalized Confusion Matrix\")\n",
    "print(\"Normalized Confusion Matrix\")\n",
    "print(disp.confusion_matrix)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(clf_tertile_rf)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "# shap.summary_plot(shap_values, X_test, plot_type=\"bar\")\n",
    "shap.summary_plot(shap_values, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Most Important Feature: ' + str(test_df.columns[221]))\n",
    "print('2nd Most Important Feature: ' + str(test_df.columns[84]))\n",
    "print('3rd Most Important Feature: ' + str(test_df.columns[61]))\n",
    "print('4th Most Important Feature: ' + str(test_df.columns[64]))\n",
    "print('5th Most Important Feature: ' + str(test_df.columns[62]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search on depth and learning rate (xgboost)\n",
    "\n",
    "param_distributions_tertile = {\n",
    "    \"max_depth\": [6, 9],\n",
    "    \"learning_rate\": [0.05, 0.1, 0.5],\n",
    "}\n",
    "xgb_tertile = XGBClassifier(n_estimators=400, verbosity=2, objective='multi:softmax', nthread=4, gamma=0.1, subsample=0.6, colsample_bytree=1.0, random_state=26)\n",
    "grid_search_tertile = GridSearchCV(estimator = xgb_tertile, param_grid = param_distributions_tertile, n_jobs = -1, cv=3, verbose=3)\n",
    "grid_search_tertile.fit(X_train, y_train)\n",
    "opt_params_tertile_xgb = grid_search_tertile.best_params_\n",
    "print(opt_params_tertile_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost implementation\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "param = {'max_depth': 9, 'eta': 0.05, 'objective': 'multi:softmax', 'num_class': 3, 'gamma': 0.2, 'subsample': 0.6, 'colsample_bytree': 1.0}\n",
    "param['nthread'] = 4\n",
    "param['verbosity'] = 3\n",
    "param['eval_metric'] = ['merror']\n",
    "evallist = [(dtest, 'eval'), (dtrain, 'train')]\n",
    "\n",
    "num_round = 400\n",
    "bst = xgb.train(param, dtrain, num_round, evallist)\n",
    "bst.save_model('0004.model')\n",
    "y_hat_test = bst.predict(dtest)\n",
    "y_hat_train = bst.predict(dtrain)\n",
    "print()\n",
    "print('Tuned XGBoost (by tertile) training accuracy: ' + str(accuracy_score(y_train, y_hat_train)))\n",
    "print('Tuned XGBoost (by tertile) testing accuracy: ' + str(accuracy_score(y_test,y_hat_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print('Tuned XGBoost (by tertile) training accuracy: ' + str(accuracy_score(y_train, y_hat_train)))\n",
    "print('Tuned XGBoost (by tertile) testing accuracy: ' + str(accuracy_score(y_test,y_hat_test)))\n",
    "xgb.plot_importance(bst, max_num_features=15)\n",
    "\n",
    "print()\n",
    "print('Most Important Feature: ' + str(test_df.columns[1]))\n",
    "print('2nd Most Important Feature: ' + str(test_df.columns[2]))\n",
    "print('3rd Most Important Feature: ' + str(test_df.columns[228]))\n",
    "print('4th Most Important Feature: ' + str(test_df.columns[12]))\n",
    "print('5th Most Important Feature: ' + str(test_df.columns[11]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_hat_test, normalize='true')\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Normalized Confusion Matrix')\n",
    "#print(\"Normalized Confusion Matrix\")\n",
    "#print(disp.confusion_matrix)\n",
    "#plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
